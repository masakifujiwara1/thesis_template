%!TEX root = ../thesis.tex

\section{再帰型ニューラルネットワーク}
再帰型ニューラルネットワーク（Recurrent Neural Network, RNN）\cite{rumelhart1986learning1,rumelhart1986learning2}は，時系列データや系列データを処理するために設計されたニューラルネットワークの一種である．RNNは，隠れ層の出力を次の時間ステップの入力として再利用することで，データの時間的依存性をモデル化する特徴を持つ．この仕組みにより，従来のニューラルネットワークでは難しかった順序情報を考慮した学習が可能になる．

RNNの基本的な構造は，入力層，隠れ層，および出力層から成る．時系列データを扱う際には，系列データの各時点での入力が順次RNNに与えられ，隠れ層が現在の入力と過去の隠れ状態を組み合わせて次の隠れ状態を生成する．この隠れ状態は過去の情報を保持する役割を担うため，時系列の依存関係を学習する際に重要である．\\数式で表すと，RNNの隠れ状態$h_t$は式\eqref{rnn}のように計算される．

\begin{equation}
h_t = f(W_{ih}x_t + W_{hh}h_{t-1} + b_n)\label{rnn}
\end{equation}
ここで，$x_t$は時刻$t$の入力，$h_{t-1}$は時刻$t-1$の隠れ状態，$W_{ih}$と$W_{hh}$はそれぞれ入力重みと隠れ層の重み行列，$b_n$はバイアス項，そして$f$は非線形活性化関数である．

RNNには，勾配消失問題\cite{hochreiter2001gradient-grad,weinleindiplomarbeit-grad,schmidhuber2015deep-grad}や勾配爆発問題といった課題がある．これらの問題は，長い系列データを扱う場合にネットワークが十分な長期依存性を学習できない原因となる．この課題を解決するために，LSTM（Long Short Term Memory）\cite{hochreiter1997long}やGRU（Gated Reccurent Unit）\cite{chung2014empirical-gru}といった改良モデルが提案されている．これらのモデルは，ゲート機構を用いて重要な情報を選択的に保持または忘却することで，長期依存性を効率的に学習できるように設計されている．

\begin{figure}[hbtp]
  \centering
 \includegraphics[keepaspectratio, scale=0.5]
      {images/RaspberryPiMouse.png}
 \caption{Neural Network}
 \label{Fig:hoge}
\end{figure}   

\newpage
