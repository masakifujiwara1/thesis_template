%!TEX root = ../thesis.tex

\section{再帰型ニューラルネットワーク}
再帰型ニューラルネットワーク（Recurrent Neural Network, RNN）\cite{rumelhart1986learning1,rumelhart1986learning2}は，時系列データや系列データを扱うために設計されたニューラルネットワークの一種である．RNNは隠れ層の出力を次の時間ステップの入力として再利用し，データの時間的依存性をモデル化できる点に特徴がある．この仕組みによって，従来のニューラルネットワークでは困難だった順序情報の学習が可能となる．

RNNの基本構造は，入力層・隠れ層・出力層から構成される．時系列データを用いる際，系列データの各時刻に対応する入力が順次RNNに与えられ，隠れ層は現在の入力と直前の隠れ状態を組み合わせて次の隠れ状態を生成する．過去の情報を保持するこの隠れ層が，長期依存関係を学習する上で重要な役割を担う．\\数式で表すと，時刻$t$における隠れ状態$h_t$は式\eqref{rnn}のように計算される．
\begin{equation}
h_t = f(W_{ih}x_t + W_{hh}h_{t-1} + b_n)\label{rnn}
\end{equation}
ここで，$x_t$は時刻$t$の入力，$h_{t-1}$は時刻$t-1$の隠れ状態，$W_{ih}$と$W_{hh}$はそれぞれ入力重みと隠れ層の重み行列，$b_n$はバイアス項，そして$f$は非線形活性化関数である．

RNNには，勾配が時間を経るにつれて減少または増大し続けることで学習が困難になる勾配消失問題\cite{hochreiter2001gradient-grad,weinleindiplomarbeit-grad,schmidhuber2015deep-grad}や勾配爆発問題が存在する．これらの問題は，長い系列データを扱う場合に十分な長期依存性を学習できない原因となる．そこで，LSTM（Long Short Term Memory）\cite{hochreiter1997long}やGRU（Gated Reccurent Unit）\cite{chung2014empirical-gru}などの改良モデルが提案されている．これらのモデルは，ゲート機構を用いて重要な情報を選択的に保持または忘却することで，長期依存性を効率よく学習するように設計されている．

\begin{figure}[hbtp]
  \centering
 \includegraphics[keepaspectratio, scale=0.5]
      {images/RaspberryPiMouse.png}
 \caption{Neural Network}
 \label{Fig:hoge}
\end{figure}   

\newpage
