%!TEX root = ../thesis.tex

\section{アテンション機構}
アテンション機構\cite{vaswani2017attention}は，ニューラルネットワークの文脈で導入された手法の一つであり，特に自然言語処理や画像認識などにおいて顕著な成果を示している．その主な目的は，入力データに含まれる重要な情報に焦点を当て，それ以外の情報を適度に省くことである．この仕組みにより，モデルはより効率的かつ効果的に学習を進めることが可能となる．

アテンション機構の基本的な考え方は，入力を構成する各要素がどの程度重要かをスコア付けし，そのスコアをもとに加重平均を行うことである．これにより，モデルはタスクにとって重要な部分に「注意」を向けることが可能となる．このスコア付けの過程では，キー（Key），クエリ（Query），バリュー（Value）の3つの成分が用いられる．それぞれの役割を以下に示す．

\begin{itemize}
  \item \textbf{クエリ（Query）}\\
  特定の出力を生成する際，入力シーケンス全体の中で「どこに注目すべきか」を判断するためのベクトルである．
  \item \textbf{キー（Key）}\\
  入力データ各部分の重要度を示す指標となるベクトルであり，クエリとの比較によって異なる部分の関連性が評価される．
  \item \textbf{バリュー（Value）}\\
  実際に計算される情報の内容を保持するベクトルである．注意スコアに基づいて加重平均が適用され，最終的な出力が得られる．
\end{itemize}

アテンション機構の計算では，まずクエリとキーの内積によって各要素間の関連性を評価し，ソフトマックス関数による正規化によって注意スコアを算出する．その後，このスコアを用いてバリューに対する加重平均を行い，最終的な結果を出力する．

この手法は，特にTransformerアーキテクチャにおいて重要な役割を果たしている．Self-Attentionとして知られる拡張では，入力シーケンスの各要素が他のすべての要素とどのように関連しているかを同時に評価する．これにより，従来のリカレントニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）\cite{fukushima1980neocognitron-cnn,lecun1989backpropagation-cnn}が抱えていた長距離依存性の問題を効果的に緩和できる．

さらにアテンション機構は，自然言語処理のみならず，画像認識や音声処理など幅広い領域に応用されている．例えば，画像セグメンテーションタスクでは，画像内の特定領域に焦点を当てることで高精度な結果が得られる．このようにアテンション機構は汎用的かつ強力な手法として，多くの応用領域でその有用性が示されている．

\begin{figure}[hbtp]
  \centering
 \includegraphics[keepaspectratio, scale=0.5]
      {images/RaspberryPiMouse.png}
 \caption{Neural Network}
 \label{Fig:hoge2}
\end{figure}   

\newpage
