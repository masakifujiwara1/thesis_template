%!TEX root = ../thesis.tex

\section{Attention機構}
Attention機構\cite{vaswani2017attention}は，ニューラルネットワークの文脈で導入された手法であり，特に自然言語処理や画像認識などのタスクにおいて顕著な成果を上げている技術である．この手法の主な目的は，入力データの中から重要な情報に焦点を当て，それ以外の情報を適度に無視することである．これにより，モデルはより効率的かつ効果的に学習を進めることが可能となる．

Attention機構の基本概念は，入力に含まれる各要素がどの程度重要であるかをスコア付けし，そのスコアを基に加重平均を計算することである．これにより，モデルは特定のタスクにおいて重要な部分に「注意」を向けることができる．このスコア付けは，キー（Key），クエリ（Query），バリュー（Value）の3つの成分を用いた計算で表現される．以下に，それぞれの構成要素について説明する．

\begin{itemize}
  \item クエリ（Query）\\
  クエリは，特定の出力を生成する際に，どの部分の情報が重要であるかを判断するための基準となるベクトルである．入力シーケンス全体に対して「どこを注目すべきか」を問う役割を果たす．
  \item キー（Key）\\
  キーは，入力データの各部分がどの程度重要であるかを示す指標となるベクトルである．クエリと比較されることで，入力内の異なる部分の関連性が評価される．
  \item バリュー（Value）\\
  バリューは，実際に計算される情報の内容を保持するベクトルである．計算された注意スコアに基づいて加重平均が適用され，最終的な出力が生成される．
\end{itemize}

Attention機構の計算では，クエリとキーの内積を用いて関連性を評価し，ソフトマックス関数で正規化することで注意スコアを得る．その後，これらのスコアを用いてバリューに加重平均を適用し，最終的な結果を出力する．

この手法は，特にTransformerアーキテクチャにおいて重要な役割を果たしている．Self-Attentionとして知られる拡張では，入力シーケンスの各要素が他のすべての要素とどのように関連しているかを同時に評価する．これにより，従来のリカレントニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）\cite{fukushima1980neocognitron-cnn,lecun1989backpropagation-cnn}の限界であった長距離依存性の問題を効果的に解決することが可能となる．

Attention機構は，自然言語処理のみならず，画像認識や音声処理といった他の分野にも応用されている．例えば，画像セグメンテーションタスクでは，画像内の特定の領域に焦点を当てることで，より精度の高い結果が得られる．このように，Attention機構は汎用的かつ強力なツールとして，多くの応用領域でその価値を証明している．

\begin{figure}[hbtp]
  \centering
 \includegraphics[keepaspectratio, scale=0.5]
      {images/RaspberryPiMouse.png}
 \caption{Neural Network}
 \label{Fig:hoge2}
\end{figure}   

\newpage
